<!DOCTYPE html>
<html>
  <head>
    <title>AskReddit Analysis</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="../css/theme.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" type="text/css">
  </head>
  <body>

    <!-- Header -->
    <div class="pages_header">
      <div class="header_item">
	<h1>Visualizing a Year of /r/AskReddit Submissions</h1>
	<h3 style="justify-content: flex-end;">Jan 29, 2019</h3>
      </div>
    </div>

    <!-- Body -->
    <div class="col space">
      <div class="title">
	<h1>AskReddit is Pretty Popular</h1>
	<p>
	  Today I was browsing Reddit (are we surprised?), and did my normal check of <a target="_blank" rel="noopener noreferrer"  href="https://www.reddit.com/r/AskReddit/">/r/AskReddit</a>, where any user can ask literally any question to the user base. AskReddit is the <a  target="_blank" rel="noopener noreferrer" href="http://redditmetrics.com/top">third most subscribed to subreddit</a> and most active subreddit (<a  target="_blank" rel="noopener noreferrer" href="https://fivethirtyeight.com/features/what-reddit-can-tell-us-about-nba-fan-bases/">according to FiveThirtyEight</a>). Since this is the single busiest subreddit, I felt like most of the submissions would go ignored and recieve few comments and few upvotes. Let's see if that's true!
	</p>
	<h2>Scrape that Data!</h2>
	<p>
	  Reddit is really great and makes it  super easy to set up your own API with. When I first started working on this, I decided to work with the Python Reddit API Wrapper (or PRAW), since it seemed fairly straight forward to set up. I wrote a short script that allowed me to scrape the top 10,000 most upvoted posts from /r/AskReddit. However, when I stored the data into a data frame, I noticed that it only had 500 rows, which is just a little shy of the 10,000 I wanted. Upon further inspection, users are limited to scraping only 500 submissions from Reddit  at a time (this was mentioned in the PRAW documention, shame on me). So after some quick googling, I found a workaround that was actually simpler using the <a target="_blank" rel="noopener noreferrer"  href="https://github.com/pushshift/api">Pushshift API</a> and help from this  <a target="_blank" rel="nooper noreferrer" href="https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4"> Medium article </a>.
	</p>

	<p>
	  Pushshift is SUPER cool because it's so simple to use. Go ahead and navigate to <a target="_blank" rel="noopener noreferrer"  href="https://api.pushshift.io/reddit/search/submission/?subreddit=askreddit">https://api.pushshift.io/reddit/search/submission/?subreddit=askreddit</a>. The webpage will display a JSON data file with the data from the 25 most recent submissions. BUT WAIT! IT GETS BETTER! Pushshift allows you to chain criteria to fit your needs. Say we wanted to grab the top 500 upvoted posts of all time for /r/askreddit. We would enter  <a target="_blank" rel="noopener noreferrer"  href="https://api.pushshift.io/reddit/search/submission/?subreddit=askreddit&limit=500&sort=dsc&sort_type=score">https://api.pushshift.io/reddit/search/submission/?subreddit=askreddit&limit=500&sort=dsc&sort_type=score</a> into the address bar, and there you have it! A lovely JSON with loads of data!
	</p>
	
	<p>
	  Pushshift is still limited in how much data it is capable of pulling though. So the next problem to tackle was to figure out how to make requests  over and over again to get more unique data. The Medium article I mentioned above helped me figure out how to use the Requests library in Python, which retrieves the JSON data from Pushshift, and the proper way to store it in a list. Here is the code I borrowed and slightly modified:
	<pre><code>def make_request(uri, max_retries = 2):
    def fire_away(uri):
        response = requests.get(uri)
        assert response.status_code == 200
        return json.loads(response.content)

    current_tries = 1
    while current_tries < max_retries:
        try:
            time.sleep(1)
            response = fire_away(uri)
            return response
        except:
            time.sleep(1)
            current_tries += 1

    return fire_away(uri)

# Here is the criteria that I want to grab from the pushshift JSON. More data can
# be added depending on what you want, just look at the pushshift documentation!			  
			  
def map_posts(posts):
        return list(map(lambda post: {
            'id': post['id'],
            'time': post['created_utc'],
            'score': post['score'],
            'comments': post['num_comments']}, posts))</code></pre>

	  It's fairly straghtforward. It gets the data from Pushshift, and grabs the id, time the submission was created, the number of upvotes (score), and the number of comments. Pushshift offers plenty more information about the submission, but this information is what I was primarily interested in. 
	</p>
	<p>
	  Now that we have the requesting capabilities, all that was left was to use a for loop and some formatting to make requests repeatedly. Pushshift allows you to specify criteria on submissions before a certain date, and this is the key to the program.  My methodology was to collect 500 submissions at a time, working backward from the start date, which was January 2nd 2019, and would terminate at Decemeber 31st, 2017. Once collected, I would retrieve the date on the last item in the list, and the next iteration collect 500 more submissions from before that date. Take a look
	  <pre><code>size = 500
before = 1546405200
after = 1514696400
subreddit = 'askreddit'
new_time = 2000000000

year = 'https://api.pushshift.io/reddit/search/submission?subreddit={}&size={}&before={}&sort_type=created_utc'
recent_posts = map_posts(
    make_request(
        year.format(subreddit, size, before))['data'])

# terminate loop when the new date passed 12/31/17
while(new_time > after):
    last = recent_posts[-1]
    new_time = last['time']

    # This prints out every iteration which I used to check progress 
    print(get_date(new_time), end = "\n")

    recent_posts.extend(
        map_posts(
            make_request(
                year.format(subreddit, size, new_time))['data']))</code></pre>

Running this code takes an eternity, so I recommend taking a nap or queueing up your favorite Netflix show! I wanted to run this data overnight while I slept so I wouldn't have to wait all day for it, so I tried three separate nights to run the script but each time it failed. So I finally ran it in the background while I was working and it took four hours! Anyway, so now that we have the data, we will utilize Pandas to make the JSON into a dataframe, and once it's in the dataframe, we will change the timestamp to a datetime and export it to a CSV!  

	<pre><code>recent_posts_df = pd.DataFrame(recent_posts)
_timestamp = recent_posts_df["time"].apply(get_date)
recent_posts_df = recent_posts_df.assign(timestamp = _timestamp)
recent_posts_df.to_csv('year.csv', index=False)</code></pre>
	</p>
	
	<h2>Let's Get Visualizing</h2>
	<p>
	  The ggplot2 package is my favorite method of visualizing data, so I imported the CSV into R. Firstly, I wanted to examine when the best time to post to AskReddit to maximize upvoting. I had to do some extra data cleaning once imported, though. For example, I had to add 7 hours to the datetime column to convert it to the EST timezone since the column's initial time zone was UTC. Similarly, I made a column that extracted the hour so I can look at upvotes by hour. It is also important to note that the first and last days in the dataset did not perfectly end at midnight, meaning there were extra hours for both of those days. I excluded those days in order to make sure each day went from midnight to midnight.
	  <pre><code>library(tidyverse)
library(lubridate)
	      
# Set the theme, super useful! 	      
theme_set(ggthemes::theme_fivethirtyeight() + theme(axis.title = element_text()))
askreddit <- read_csv("~/Documents/Python/year.csv")

askreddit <- askreddit %>%
  # Convert UTC to EST and make hour column
	 mutate(timestamp = timestamp + dhours(7),    
         hour = hour(timestamp))

# Trim off those extra days on the start and end 
first_date <- date(askreddit$timestamp[1])
last_date <- date(askreddit$timestamp[nrow(askreddit)])

askreddit <- askreddit %>% 
  arrange(timestamp) %>% 
  filter(date(timestamp) != first_date, date(timestamp) != last_date)

# Trim one more cause I'm dumb
last_trimmed_date <- date(askreddit$timestamp[nrow(askreddit)])

askreddit <- askreddit %>% 
  arrange(timestamp) %>% 
  filter(date(timestamp) != last_trimmed_date)</code></pre>


Something that I noticed further on in my exploration was that posts that were made in October through December did not have the correct number of upvotes.Great! Now we have everything we need to make some plots. We'll start off by seeing the mean number of upvotes for posts for every hour.
	      <pre><code>askreddit %>% 
  group_by(hour) %>% 
  summarise(mean = mean(score)) %>% 
  ggplot(aes(hour, mean))+
  geom_bar(stat = "identity") + 
  labs(x = "Hour (EST)",
       y = "Average Upvotes",
       title = "When do Submissions to /r/AskReddit Receive the Most Upvotes on Average?",
       caption = "@Joe_Stoica")+
  scale_x_continuous(breaks = seq(0,23,1))+
  scale_y_continuous(breaks = seq(0, 60, 10))+
  theme(plot.title = element_text(size = 14))</code></pre>	  
	</p>

	<div style="text-align: center">
	  <img src="../images/hourly_bar.png" width="85%"/>
	</div>
	<p>
	  It looks like that submissions in the afternoon get the most upvotes, peaking around 2:00pm. However, it's also worth mentioning that the mean is probably not extremely accurate as a measure of centrality  due to the high right skew of upvotes, but the median value of upvotes for every hour was 1 so that didn't offer much insight (but more on that later). It's difficult to discern why this happens exactly. The United States <a target="_blank" rel="nooper noreferrer"  href="https://www.statista.com/statistics/325144/reddit-global-active-user-distribution/">accounts for 40%</a> of all Reddit traffic, and the US is split across four time zones. I would guess that lunch breaks would result in more browsing, as well as the afternoon lull where bored people might want to browse reddit.  I wanted to examine when posts were made to see if submissions were less saturated in the afternoon, which could potentially let posts get more visibility, resulting in higher upvote concentrations.  
	<pre><code>askreddit %>% 
   ggplot(aes(factor(hour)))+
   geom_bar() + 
   labs(x = "Hour (EST)",
        y = "Submissions Count",
        title = "When do Submissions to /r/AskReddit Get Posted Most Often?",
        caption = "@Joe_Stoica")+
   scale_y_continuous(breaks = seq(0, 50000, 10000)) + 	      
   theme(plot.title = element_text(size = 14))</code></pre>
	</p>
	
	<div style="text-align: center">
	  <img src="../images/post_time.png" width="85%"/>
	</div>
	<p>
	  Interestingly, we do see a decrease in submissions around noon, and a steady increase as the evening progresses, however I doubt there's any  
	<p>
      </div>
    </div>
    
    
    <div class="footer">
       <div>  
	 <a target="_blank" rel="noopener noreferrer" href="../index.html" target="blank"><i class="fa fa-home fa-3x"></i></a>
       </div>
	<div>
	  <a target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/joe-stoica/" target="blank"><i class="fa fa-linkedin fa-3x"></i></a>
	</div>
	<div>
	  <a target="_blank" rel="noopener noreferrer" href="https://twitter.com/Joe_Stoica" target="blank"><i class="fa fa-twitter fa-3x"></i></a>
	</div>
	<div>
	  <a target="_blank" rel="noopener noreferrer" href="https://github.com/joestoica" target="blank"><i class="fa fa-github fa-3x"></i></a>
	</div>
	<div>
	  <a target="_blank" rel="noopener noreferrer" href="mailto:joe.stoica@gmail.com" target="blank"><i class="fa fa-envelope fa-3x"></i></a>
	</div>
      </div>

  </body>
</html>
