<!DOCTYPE html>
<html lang="en">
  <head>
    
      <title>Using 1.6 Million Tweets with a Markov Chain Text Generator :: Joe Stoica</title>
    
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="description" content="During my daily dose of reddit browsing, I came across this post and was subsequently pretty confused. How can an office be THAT high? Amidst my bewilderment, I noticed that it came from the subreddit
SubredditSimulator, and its hilarity immediately grabbed my attention. If you are unfamiliar with how Reddit operates, it is basically a giant forum split into thousands of subcommunities called &amp;ldquo;subreddits.&amp;rdquo; Users are allowed to subscribe to whatever subreddits they want to so they can regularly view its content, create posts in any subreddit, and comment on any post they like."/>
<meta name="keywords" content=""/>
<meta name="robots" content="noodp"/>
<link rel="canonical" href="https://joestoica.com/work/markov_chain/" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" type="text/css">

<style>
    .item {
    padding: 10px;
    }

    .foot {
        color: #fff;
    }

</style>





<link rel="stylesheet" href="https://joestoica.com/assets/style.css">


<link rel="stylesheet" href="https://joestoica.com/style.css">


<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://joestoica.com/img/apple-touch-icon-144-precomposed.png">
<link rel="shortcut icon" href="https://joestoica.com/img/favicon.png">


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Using 1.6 Million Tweets with a Markov Chain Text Generator"/>
<meta name="twitter:description" content="During my daily dose of reddit browsing, I came across this post and was subsequently pretty confused. How can an office be THAT high? Amidst my bewilderment, I noticed that it came from the subreddit
SubredditSimulator, and its hilarity immediately grabbed my attention. If you are unfamiliar with how Reddit operates, it is basically a giant forum split into thousands of subcommunities called &ldquo;subreddits.&rdquo; Users are allowed to subscribe to whatever subreddits they want to so they can regularly view its content, create posts in any subreddit, and comment on any post they like."/>



<meta property="og:title" content="Using 1.6 Million Tweets with a Markov Chain Text Generator" />
<meta property="og:description" content="During my daily dose of reddit browsing, I came across this post and was subsequently pretty confused. How can an office be THAT high? Amidst my bewilderment, I noticed that it came from the subreddit
SubredditSimulator, and its hilarity immediately grabbed my attention. If you are unfamiliar with how Reddit operates, it is basically a giant forum split into thousands of subcommunities called &ldquo;subreddits.&rdquo; Users are allowed to subscribe to whatever subreddits they want to so they can regularly view its content, create posts in any subreddit, and comment on any post they like." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://joestoica.com/work/markov_chain/" />
<meta property="article:published_time" content="2019-02-19T18:18:25-05:00" />
<meta property="article:modified_time" content="2019-02-19T18:18:25-05:00" /><meta property="og:site_name" content="Joe Stoica" />






  </head>
  <body class="dark-theme">
    <div class="container">
      <header class="header">
  <span class="header__inner">
    <a href="/about" class="logo" style="text-decoration: none;">
  
    <span class="logo__mark"><svg xmlns="http://www.w3.org/2000/svg" class="greater-icon" viewBox="0 0 44 44">
  <path fill="none" d="M15 8l14.729 14.382L15 35.367"/>
</svg>
</span>
    <span class="logo__text">Joe Stoica</span>
    <span class="logo__cursor"></span>
  
</a>

    <span class="header__right">
      
        <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="/now">Now</a></li>
        
      
        
          <li><a href="/work">Work</a></li>
        
      
      
    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about">About</a></li>
      
    
      
        <li><a href="/now">Now</a></li>
      
    
      
        <li><a href="/work">Work</a></li>
      
    
  </ul>
</nav>

        <span class="menu-trigger">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M0 0h24v24H0z" fill="none"/>
            <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
          </svg>
        </span>
      
      <span class="theme-toggle">
        <svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>

      </span>
    </span>
  </span>
</header>


      <div class="content">
        
  
  

  <div class="post">
    <h2 class="post-title"><a href="https://joestoica.com/work/markov_chain/">Using 1.6 Million Tweets with a Markov Chain Text Generator</a></h2>
    <div class="post-meta">
      
        <span class="post-date">
          2019-02-19
        </span>

        
          
        
      

      
      
    </div>

    
      <span class="post-tags">
        
          #<a href="https://joestoica.com/tags/r/">R</a>&nbsp;
        
          #<a href="https://joestoica.com/tags/projects/">projects</a>&nbsp;
        
      </span>
    

    

    <div class="post-content">
      <p>During my daily dose of reddit browsing, I came across <a href="https://www.reddit.com/r/SubredditSimulator/comments/91pjrh/the_view_from_my_offices_toilet_today/">this post</a>
and was subsequently pretty confused. <em>How can an office be THAT high?</em> Amidst
my bewilderment, I noticed that it came from the subreddit<br>
<a href="https://www.reddit.com/r/SubredditSimulator/">SubredditSimulator</a>, and its
hilarity immediately grabbed my attention. If you are unfamiliar with how Reddit
operates, it is basically a giant forum split into thousands of subcommunities
called &ldquo;subreddits.&rdquo; Users are allowed to subscribe to whatever subreddits they
want to so they can regularly view its content, create posts in any subreddit,
and comment on any post they like.</p>
<p>But /r/SubredditSimulator is a little different. Regular users can&rsquo;t make their
own posts or comments, however they still play a role in the voting system for
content. The users that generate the actual content  are all bots who use
<a href="https://en.wikipedia.org/wiki/Markov_chain#Markov_text_generators">Markov Chain text generation</a>
to create their content, hence the seemingly nonsensical titles and comments.
The bots are all modeled after a certain subreddit, and it&rsquo;s text data comes
solely from their home. For example, /u/Jokes_SS is meant to mimic posts and
comments found on /r/jokes.</p>
<h2 id="but-what-is-a-markov-chain">But what is a Markov Chain?</h2>
<p>Basically, a Markov Chain is a way to model a stochastic process of one state
transitioning to its next state. It is important to note that future outcomes
only depend on the current state of the model, and not past events (this is the
memoryless property). Wikipedia uses this image to  illustrate this idea:</p>
<p><img src="/images/markov_chain.png" alt="https://en.wikipedia.org/wiki/Examples_of_Markov_chains "></p>
<p>The two nodes on this graph (sunny or rainy) represent two states that the model
can be in, while the edges and their respective numbers represent the probabilty
of the transition from one state to the next (in this case the probability of
the forecast on the following day). Let&rsquo;s say today is Sunny. There is a 90%
chance it will be sunny on the tomorrow, while there is a 10% chance  tomorrow
will be rainy. It doesn&rsquo;t matter if yesterday was sunny or rainy, because today
it&rsquo;s sunny, and that&rsquo;s all that matters when considering tomorrow&rsquo;s weather.</p>
<p>This same process can be applied to text generation processes. Let&rsquo;s make some
data!</p>
<pre><code>1. &quot;I pet the dog.&quot;&lt;br&gt;
2. &quot;I have a pet dog.&quot;&lt;br&gt;
3. &quot;I went to the pet store.&quot;&lt;br&gt;
4. &quot;I pet the pet cat.&quot;&lt;br&gt;
</code></pre>
<p>Great! now let&rsquo;s build a sentence. We will start with the letter &ldquo;I&rdquo; for
simplicity purposes.</p>
<pre><code>I
</code></pre>
<p>If we fed this into a Markov Chain function, it would build a data structure
that has a key word (in this case it is &ldquo;I&rdquo;), and it&rsquo;s values would be all of
the words that followed the word &ldquo;I&rdquo; in our given data.</p>
<pre><code>1. &quot;I &lt;mark&gt;pet&lt;/mark&gt; the dog.&quot;&lt;br&gt;
2. &quot;I &lt;mark&gt;have&lt;/mark&gt; a pet dog.&quot;&lt;br&gt;
3. &quot;I &lt;mark&gt;went&lt;/mark&gt; to the pet store.&quot;&lt;br&gt;
4. &quot;I &lt;mark&gt;pet&lt;/mark&gt; the pet cat.&quot;&lt;br&gt;
</code></pre>
<p>Once all of the words are found, the function calculates a proportion of how
often the second word follows the first word. Using the weather example still,
the current word we are using in our sentence would be equivalent to today&rsquo;s
weather, and the next word would be tomorrow&rsquo;s weather. So since we started with
&ldquo;I&rdquo;, the words that follow it in our data are &ldquo;pet&rdquo;, &ldquo;have&rdquo;, &ldquo;went&rdquo;, and &ldquo;pet&rdquo;
again. The data might have this sort of map-esque structure (note that this is
just the &ldquo;I&rdquo; data):</p>
<pre><code>{&quot;I&quot;: {&quot;pet&quot;:0.5, &quot;went&quot;:0.25, &quot;have&quot;:0.25}, ...}
</code></pre>
<p>So we have the word we are currently using at the front, and then each
successive word and its probabilities. The function would then randomly choose a
word using these probabilities, and then add it to the sentence. Let&rsquo;s say it
chose &ldquo;pet&rdquo;, so it&rsquo;s added after &ldquo;I.&rdquo;</p>
<pre><code>I pet
</code></pre>
<p>It  then creates the transition map for &ldquo;pet&rdquo;:</p>
<pre><code>{&quot;pet&quot;:{&quot;the&quot;:0.4, &quot;dog.&quot;:0.2, &quot;store.&quot;:0.2, &quot;cat.&quot;:0.2}}
</code></pre>
<p>and randomly selects a work and adds to our sentence:</p>
<pre><code>I pet store.
</code></pre>
<p>It doesn&rsquo;t really make sense, but that&rsquo;s why it&rsquo;s so fun. Now that we understand
the basics of Markov Chains, let&rsquo;s open up R and implement it there.</p>
<h2 id="r-implementation">R Implementation</h2>
<p>We will make use of the <a href="https://cran.r-project.org/web/packages/ngram/ngram.pdf">ngram R package</a>
because it has a Markov Chain function named babble() that we will make use of.
Our data comes from <a href="https://www.kaggle.com/kazanova/sentiment140">this Kaggle dataset</a>,
which contains 1.6 million tweets, which is great for large amounts of word pair
associations to create, however there is a trade-off in how long it will take
for our script to run. Small datasets are not ideal for generators simply
because there will not be as many word pairs to randomly choose from, and
sentences will look very similar to the original ones fed into the function.
Here is a script I wrote to return a tweet using this data.</p>
<pre><code>library(tidyverse)
library(ngram)
library(stringi)

tweets &lt;- read_csv(&quot;training.1600000.processed.noemoticon.csv&quot;, col_names =
    c(&quot;target&quot;, &quot;ids&quot;, &quot;date&quot;, &quot;flag&quot;, &quot;user&quot;, &quot;text&quot;))

# Encode weird byte issues so it concatenate actually runs
tweets$text &lt;- stri_enc_toutf8(tweets$text, is_unknown_8bit = TRUE, validate = TRUE)

# Eliminate tweets longer than the max character limit
tweets &lt;- tweets %&gt;% filter(nchar(text) &lt;= 280)
# Turn the column containing all of the tweets into one
# string to be fed into the babble function
# This takes a while to run due to the size of the data.
# This took my 2015 Macbook Pro  51.25 seconds to run, but your mileage may vary
str &lt;- concatenate(tweets$text)

# Create the ngram obejct to be passed into the babble function
ng &lt;- ngram(str)

# This function takes an ngram object and an integer that specifies number of words to return.
# The trim argument will trim off excessive words after the
# last punctuation point, and makes the tweet a little more understandable.

# Input: ngram object, integer, boolean
# Output: String

make_tweet &lt;- function(ngram = ng, num = 35, trim = TRUE){
  stopifnot(num &lt;= 35)

  # Generate Tweet
  str &lt;- babble(ngram, genlen = num)

  # remove mentions, hastags, links, and fix quote and ampersand encoding issue
  search_patterns &lt;- c(&quot;@\\w+ *&quot;, &quot;#\\w+ *&quot;, &quot;https\\w+ *&quot;,
           &quot;http\\w+ *&quot;, &quot;Http\\w+ *&quot;, &quot;HTTP\\w+ *&quot;,
                       &quot;www.\\w+ *&quot;,&quot;://t.co/\\w+)&quot;, &quot;://bit.ly/\\w+ *&quot;,
                       &quot;&amp;quot;&quot;, &quot;&amp;amp&quot;)

  replace_patterns &lt;- c(rep(&quot;&quot;, length(search_patterns) - 1), &quot;and&quot;)

# This website helped me with mapply
# https://bit.ly/2T7Rz5Q

  mapply(function(u, v) str&lt;&lt;-gsub(u, v, str), search_patterns, replace_patterns)

  # Cut off sentence fragments at end.
  if (trim == TRUE) {str &lt;- gsub(x = str, pattern = &quot;[^.]+$&quot;, &quot;&quot;)}

  # Capitalize first letter
  substr(str, 0, 1) &lt;- str_to_upper(substr(str, 0, 1))

  # Remake tweet if too long, too short, or empty
  if (nchar(str) &gt; 280 || nchar(str) &lt;= 5 || nchar(str) == 0) {
    # Reset str so it doesn't print
    str = &quot;&quot;
    make_tweet(ngram, num)
  }

  # Print it!
  if (str != &quot;&quot;) {
    print(noquote(str))
  }
}

make_tweet()
</code></pre>
<p>Let&rsquo;s see some tweets that it generated!</p>
<pre><code>Know!! I wish I were him I'd rather stay with them but then again, i'm dreading thursday why doesn't that count? i would like to hear any insults! My guitar is in LA

So. Thankfully Disney channel offers wonderful programming for multi-processor systems! I think I'm immune to the park with cath...what a beautiful day outside watchin some unfabulous episodes... i couldnt sleep last night too.

Iz was fussing all night xxx i'm just so i told my exam now. crap. i forgot about bones even though it's night at ours... kids, popcorn, cosy slippers, life is short...
</code></pre>
<h2 id="its-as-simple-as-thath2">It&rsquo;s as simple as that!<!-- raw HTML omitted --></h2>
<p>Everything looks like it is working properly, which is always nice. I hope you
enjoyed this post, feel free to contact me via Twitter or email if you have any
questions or feedback. I would love to see other ideas or fun data sets to use
with this code.</p>
<p>*Disclaimer: The content and ideas expressed in the tweets are not my own.</p>

    </div>
    
      
        <div class="pagination">
          <div class="pagination__title">
            <span class="pagination__title-h">Read other posts</span>
            <hr />
          </div>
          <div class="pagination__buttons">
            
              <span class="button previous">
                <a href="https://joestoica.com/work/xkcd/">
                  <span class="button__icon">←</span>
                  <span class="button__text">Viewing xkcd comics in RStudio</span>
                </a>
              </span>
            
            
              <span class="button next">
                <a href="https://joestoica.com/work/538/">
                  <span class="button__text">Remaking Visualizations from 538</span>
                  <span class="button__icon">→</span>
                </a>
              </span>
            
          </div>
        </div>
      
    


    
      
        

      
    

    </div>

      </div>

      
          <div class="foo">
    <a class="item" href="https://www.linkedin.com/in/joe-stoica/"><i class="fa fa-linkedin fa-2x"></i></a>
    <a class="item" href="https://twitter.com/Joe_Stoica"><i class="fa fa-twitter fa-2x"></i></a>
    <a class="item" href="https://github.com/joestoica"><i class="fa fa-github fa-2x"></i></a>
    <a class="item" href="mailto:contactjoestoica@gmail.com"><i class="fa fa-envelope fa-2x"></i></a>
  </div>

<script src="https://joestoica.com/assets/main.js"></script>
<script src="https://joestoica.com/assets/prism.js"></script>

<script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
           extensions: ["AMSmath.js", "AMSsymbols.js"] }
    }
    });
    MathJax.Hub.Queue(function() {
      
      
      
      var all = MathJax.Hub.getAllJax(), i;
      for(i = 0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });

    MathJax.Hub.Config({
    
    TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
  </script>

</html>

      
    </div>

    
  </body>
</html>
